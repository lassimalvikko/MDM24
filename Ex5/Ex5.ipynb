{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from string import punctuation\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/vilmatiainen/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vilmatiainen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = 'data/scopusabstracts.csv'\n",
    "reader = open(data_path, 'r', encoding='utf-8')\n",
    "reader.readline() # skip header\n",
    "lines = reader.readlines()\n",
    "text = [i.split('#')[1] + ' ' + i.split('#')[2] for i in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline solution. Set False for improved solution\n",
    "baseline = False\n",
    "# Run failed attempts at improvement\n",
    "failed2 = False # LSA\n",
    "\n",
    "# tokenization\n",
    "tokens_list = [word_tokenize(i) for i in text]\n",
    "\n",
    "# lowercasing\n",
    "lc_tokens_list = []    \n",
    "for i in tokens_list: \n",
    "    lc_tokens_list.append([token.lower() for token in i])\n",
    "\n",
    "# removing stopwords, punctuation, and numbers\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(punctuation)\n",
    "stop_words.add(\"...\")\n",
    "\n",
    "if not baseline:\n",
    "    # Adding stopwords which were problematic in later steps\n",
    "    stop_words.add(\"also\")\n",
    "    stop_words.update([\"use\", \"used\", \"uses\", \"using\"])\n",
    "    stop_words.update([\"paper\", \"papers\"])\n",
    "    stop_words.update([\"result\", \"results\", \"resulted\", \"resulting\"])\n",
    "    stop_words.update([\"base\", \"based\"])\n",
    "\n",
    "filtered_sentence = []\n",
    "for i in lc_tokens_list: \n",
    "    filtered_sentence.append([token for token in i if token not in stop_words])\n",
    "filtered_sentence = [' '.join(i) for i in filtered_sentence]\n",
    "filtered_sentence = [re.sub(r'\\d+', '', sentence) for sentence in filtered_sentence]\n",
    "\n",
    "if not baseline:\n",
    "    # Transform hyphenated terms like state-of-the-art to stateoftheart and remove '-based' from terms like 'CS-based'\n",
    "    filtered_sentence = [t.replace('-based', '').replace('-', '') for t in filtered_sentence]\n",
    "\n",
    "# stemming\n",
    "porter = PorterStemmer()\n",
    "stemmed_tokens_list = []\n",
    "for i in filtered_sentence:\n",
    "\tstemmed_tokens_list.append([porter.stem(j) for j in i.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ten rows of data after preprocessing:\n",
      "anomali detect wide area imageri geniş alan görüntülerind anomali tespiti studi detect anomali wide area imageri collect aircraft set anomali identifi anyth normal cours action purpos two differ data set experi carri data set anomali detect convolut neural network model tri gener next imag past imag design imag preprocess given model anomali detect perform compar estim imag true imag  \n",
      "person reidentif deep kroneckerproduct match groupshuffl random walk person reidentif reid aim robustli measur visual affin person imag wide applic intellig surveil associ person imag across multipl camera gener treat imag retriev problem given probe person imag affin probe imag galleri imag pg affin rank retriev galleri imag exist two main challeng effect solv problem person imag usual show signific variat differ person pose view angl spatial layout correspond person imag therefor vital inform tackl problem stateoftheart method either ignor spatial variat util extra pose inform handl challeng exist person reid method rank galleri imag consid pg affin ignor affin galleri imag gg affin affin could provid import clue accur galleri imag rank util postprocess stage current method articl propos unifi endtoend deep learn framework tackl two challeng handl viewpoint pose variat compar person imag propos novel kroneck product match oper match warp featur map differ person compar warp featur map accur pg affin fulli util avail pg gg affin accur rank galleri person imag novel groupshuffl random walk oper propos kroneck product match groupshuffl random walk oper endtoend trainabl shown improv learn visual featur integr deep learn framework propos approach outperform stateoftheart method market cuhk dukemtmc dataset demonstr effect gener abil propos approach code avail http //github.com/yantaoshen/kpm_rw_person_reid  \n",
      "crack detect imag masonri cnn signific bodi research crack detect comput vision method concret asphalt less attent given masonri train convolut neural network cnn imag brick wall built laboratori environ test abil detect crack imag brickandmortar structur laboratori realworld imag taken internet compar perform cnn varieti simpler classifi oper handcraft featur find cnn perform better domain adapt laboratori realworld imag simpl model howev find perform significantli better perform revers domain adapt task simpl classifi train realworld imag test laboratori imag work demonstr abil detect crack imag masonri varieti machin learn method provid guidanc improv reliabl model perform domain adapt crack detect masonri  \n",
      "toward energi effici code gener mobil phone smartphon becom part everyday life last year devic help us mani area life sport job weather etc sometim annoy batteri life time import find solut reduc energi consumpt smartphon one possibl method 'comput offload part process execut remot devic e.g cloud lot exampl alreadi shown comput offload reduc energi usag mobil devic howev amount energi save may differ decis make offload process control sever techniqu decis make theori schedul theori go introduc new system call ecgm energi effici code gener mobil phone ecgm decid automat compil time task run smartphon task offload benefit system demonstr measur energyeffici schedul techniqu  \n",
      "subpolyhedr schedul unit twovariableperinequ polyhedra polyhedr compil success design implement complex loop nest optim parallel compil algorithm complex scalabl limit remain one import weak address subpolyhedr underaproxim system constraint affin schedul problem propos subpolyhedr schedul techniqu unit twovariableperinequ u tvpi polyhedra techniqu reli simpl polynomi time algorithm underapproxim gener polyhedron u tvpi polyhedra modifi stateoftheart pluto compil schedul techniqu show major polybench . kernel underapproxim yield polyhedra nonempti solv underapproxim system lead asymptot gain complex show practic signific improv compar tradit lp solver verifi code gener subpolyhedr parallel prototyp match perform plutooptim code underapproxim preserv feasibl copyright  \n",
      "extract multipl viewpoint model relat databas much time process mine project spent find understand data sourc extract event data need fraction time spent actual appli techniqu discov control predict busi process moreov current process mine techniqu assum singl case notion howev reallif process often differ case notion intertwin exampl event order handl process may refer custom order order line deliveri payment therefor propos multipl viewpoint mvp model relat event object relat activ class requir event data much closer exist relat databas mvp model provid holist view process allow extract classic event log differ viewpoint way exist process mine techniqu viewpoint without need new data extract transform provid toolchain allow discoveri mvp model annot perform frequenc inform relat databas moreov demonstr classic process mine techniqu appli select viewpoint  \n",
      "program checker lexic analysi gnu c compil theori program check establish wellsuit method construct formal correct compil frontend never prove practic reallif compil proof necessari establish check method choic implement compil correctli show lexic analysi gnu c compil formal specifi check within theorem prover isabelle/hol util program check therebi demonstr formal specif verif techniqu abl handl reallif compil  \n",
      "advanc visual object track algorithm correl filter 基于相关滤波的视觉目标跟踪算法新进展 excel comprehens perform correl filter track algorithm becom hotspot theoret research practic applic field visual object track despit mani studi still lack systemat analys exist correl filter track algorithm level track framework therefor start basic framework object track algorithm characterist correl filter track algorithm deepli analyz basic problem work stage present basi main technolog progress correl filter track algorithm characterist correspond algorithm recent ten year summar typic correl filter track algorithm evalu analyz final outstand issu urgent solv futur research direct correl filter track algorithm discuss  \n",
      "studi variou databas model relat graph hybrid databas relat databas popular databas store variou type inform due everincreas growth data becom hard maintain process databas graph model becom popular sinc store handl big data effici compar relat databas relat databas graph databas advantag disadvantag overcom limit combin make hybrid model discuss relat databas graph databas advantag applic talk hybrid model  \n",
      "better report studi artifici intellig consortai beyond increas number studi artifici intellig ai publish dental oral scienc report aspect studi suffer rang limit standard toward report like recent publish consolid standard report trial consort ai extens help improv studi emerg field journal dental research jdr encourag author review reader adher standard notabl though wide rang aspect beyond report locat along variou step ai lifecycl consid conceiv conduct report evalu studi ai dentistri  \n",
      "\n",
      "The number of tokens after preprocessing is 9764.\n",
      "\n",
      "Most common words (total 9764):\n",
      "[('data', 1240), ('system', 1208), ('propos', 1082), ('model', 944), ('method', 880), ('comput', 871), ('robot', 808), ('imag', 800), ('perform', 776), ('algorithm', 721), ('databas', 701), ('secur', 665), ('approach', 622), ('compil', 611), ('applic', 593), ('learn', 574), ('design', 569), ('gener', 550), ('develop', 535), ('detect', 516), ('process', 512), ('.', 512), ('network', 511), ('inform', 508), ('present', 504), ('implement', 482), ('differ', 470), ('relat', 450), ('improv', 445), ('provid', 441), ('show', 437), ('optim', 437), ('techniqu', 430), ('time', 428), ('studi', 417), ('program', 417), ('evalu', 386), ('vision', 384), ('effici', 381), ('work', 374), ('problem', 366), ('object', 365), ('analysi', 365), ('scheme', 358), ('featur', 350), ('research', 348), ('new', 348), ('control', 337), ('key', 336), ('structur', 333), ('requir', 333), ('compar', 332), ('two', 324), ('task', 320), ('framework', 316), ('encrypt', 313), ('code', 304), ('howev', 304), ('effect', 303), ('technolog', 302), ('oper', 298), ('one', 296), ('test', 294), ('languag', 293), ('set', 281), ('achiev', 280), ('quantum', 277), ('queri', 274), ('dataset', 272), ('environ', 270), ('train', 269), ('measur', 267), ('visual', 264), ('challeng', 261), ('complex', 259), ('exist', 258), ('accuraci', 256), ('function', 255), ('architectur', 252), ('cryptographi', 252), ('integr', 241), ('increas', 241), ('experi', 240), ('demonstr', 239), ('map', 238), ('multipl', 235), ('reduc', 235), ('construct', 235), (\"'s\", 233), ('deep', 232), ('number', 231), ('user', 231), ('transform', 228), ('estim', 226), ('graph', 225), ('attack', 223), ('commun', 223), ('type', 213), ('appli', 212), ('includ', 212)]\n"
     ]
    }
   ],
   "source": [
    "# show processed data\n",
    "print('First ten rows of data after preprocessing:')\n",
    "for i in stemmed_tokens_list[:10]:\n",
    "\tfor j in i:\n",
    "\t\tprint(j,end=\" \")\n",
    "\tprint(\" \")\n",
    "\n",
    "# number of tokens\n",
    "uniques = np.unique([tok for doc in stemmed_tokens_list for tok in doc])\n",
    "print(\"\\nThe number of tokens after preprocessing is {}.\".format(len(uniques)))\n",
    "\n",
    "# check most frequent words\n",
    "listofall = [item for elem in stemmed_tokens_list for item in elem]\n",
    "freq = FreqDist(listofall)\n",
    "wnum = freq.B()\n",
    "print(\"\\nMost common words (total %d):\"%wnum)\n",
    "print(freq.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of samples is 1143 and the number of features is 3654.\n"
     ]
    }
   ],
   "source": [
    "cleaned_documents = [ ' '.join(i) for i in stemmed_tokens_list]\n",
    "ngram_range = (1, 2)\n",
    "if not baseline: # Add trigrams\n",
    "  ngram_range = (1, 3)\n",
    "tfidf_vectorizer = TfidfVectorizer(smooth_idf = False, ngram_range = ngram_range , min_df = 5, max_df = 0.8, norm='l2')\n",
    "\n",
    "tfidf_vectorizer.fit(cleaned_documents)\n",
    "tf_idf_vectors = tfidf_vectorizer.transform(cleaned_documents)\n",
    "\n",
    "# Run LSA (worsens results so failed attempt)\n",
    "if not baseline and failed2:\n",
    "  lsa = make_pipeline(TruncatedSVD(n_components=100), Normalizer(copy=False))\n",
    "  tf_idf_vectors = csr_matrix(lsa.fit_transform(tf_idf_vectors))\n",
    "\n",
    "print(f\"\\nThe number of samples is {tf_idf_vectors.shape[0]} and the number of features is {tf_idf_vectors.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buckshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buckshot(X, n_clusters, random_state):\n",
    "  n_seeds = int(np.sqrt(X.shape[0]*n_clusters))\n",
    "  idx = random_state.randint(X.shape[0], size=n_seeds)\n",
    "  agg_data = X.toarray()[idx, :]\n",
    "  agg = AgglomerativeClustering(n_clusters=n_clusters, metric='cosine', linkage='complete').fit(agg_data)\n",
    "  \n",
    "  # Use pandas to group by labels and get means\n",
    "  agg_df = pd.DataFrame(agg_data)\n",
    "  agg_df['label'] = agg.labels_\n",
    "\n",
    "  means_df = agg_df.groupby('label').mean()\n",
    "  return means_df.values # return numpy array\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding best K for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Davies-Bouldin index is 5.7921 with K = 7.\n"
     ]
    }
   ],
   "source": [
    "k_values = range(3, 11)\n",
    "db_scores = []\n",
    "labels_list = []\n",
    "\n",
    "for k in k_values:\n",
    "    if baseline:\n",
    "        init = 'k-means++'\n",
    "    else:\n",
    "        init = buckshot\n",
    "    kmeans = KMeans(n_clusters = k, random_state = 42, init=init)\n",
    "    labels = kmeans.fit_predict(tf_idf_vectors)\n",
    "    labels_list.append(labels)\n",
    "    db_score = davies_bouldin_score(tf_idf_vectors.toarray(), labels)\n",
    "    db_scores.append(db_score)\n",
    "\n",
    "i_best = np.argmin(db_scores)\n",
    "best_k = k_values[i_best]\n",
    "print(f\"The best Davies-Bouldin index is {min(db_scores):.4f} with K = {best_k}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent unigrams and bigrams for cluster 0.\n",
      "Unigrams: [('propos', 214), ('method', 210), ('comput', 198), ('imag', 189), ('vision', 187), ('perform', 178), ('model', 177), ('detect', 159), ('learn', 158), ('system', 157)]\n",
      "Bigrams: [('comput vision', 153), ('neural network', 92), ('deep learn', 84), ('convolut neural', 63), ('propos method', 53), ('machin learn', 42), ('object detect', 31), ('network cnn', 26), ('artifici intellig', 25), ('experiment show', 23)]\n",
      "Trigrams: [('convolut neural network', 63), ('neural network cnn', 26), ('deep learn model', 16), ('comput vision techniqu', 15), ('comput vision cv', 12), ('deep learn method', 11), ('unman aerial vehicl', 11), ('deep convolut neural', 11), ('comput vision system', 11), ('comput vision method', 10)]\n",
      "\n",
      "The most frequent unigrams and bigrams for cluster 1.\n",
      "Unigrams: [('databas', 165), ('relat', 137), ('data', 133), ('system', 97), ('propos', 75), ('queri', 75), ('inform', 71), ('approach', 69), ('model', 64), ('process', 59)]\n",
      "Bigrams: [('relat databas', 124), ('inform system', 29), ('databas manag', 26), ('manag system', 22), ('data model', 16), ('sql queri', 15), ('big data', 14), ('data store', 13), ('databas system', 13), ('queri languag', 13)]\n",
      "Trigrams: [('databas manag system', 21), ('relat databas manag', 15), ('relat databas rdb', 10), ('databas relat databas', 9), ('data relat databas', 9), ('relat databas system', 8), ('model relat databas', 7), ('ontolog relat databas', 7), ('store relat databas', 7), ('manag system rdbm', 7)]\n",
      "\n",
      "The most frequent unigrams and bigrams for cluster 2.\n",
      "Unigrams: [('quantum', 34), ('comput', 20), ('compil', 19), ('algorithm', 17), ('present', 16), ('gate', 15), ('state', 15), ('implement', 14), ('optim', 13), ('qubit', 13)]\n",
      "Bigrams: [('quantum comput', 18), ('quantum algorithm', 9), ('quantum gate', 7), ('compil quantum', 7), ('quantum circuit', 7), ('quantum program', 6), ('execut time', 6), ('quantum compil', 6), ('key distribut', 6), ('quantum cryptographi', 6)]\n",
      "Trigrams: [('quantum key distribut', 5)]\n",
      "\n",
      "The most frequent unigrams and bigrams for cluster 3.\n",
      "Unigrams: [('robot', 165), ('system', 80), ('control', 77), ('perform', 75), ('present', 73), ('develop', 68), ('demonstr', 60), ('propos', 60), ('design', 58), ('approach', 57)]\n",
      "Bigrams: [('robot system', 21), ('soft robot', 19), ('mobil robot', 12), ('autonom robot', 11), ('robot arm', 10), ('neural network', 10), ('develop robot', 9), ('propos approach', 9), ('robot perform', 9), ('case studi', 8)]\n",
      "Trigrams: [('random control trial', 5), ('case studi demonstr', 3), ('neural network model', 2), ('machin learn ml', 2), ('recurr neural network', 2), ('experi show propos', 2), ('artifici neural network', 2), ('unman aerial vehicl', 2), ('machin learn algorithm', 2), ('propos new approach', 1)]\n",
      "\n",
      "The most frequent unigrams and bigrams for cluster 4.\n",
      "Unigrams: [('compil', 172), ('program', 114), ('present', 88), ('comput', 81), ('languag', 77), ('code', 76), ('implement', 76), ('optim', 74), ('show', 71), ('system', 67)]\n",
      "Bigrams: [('program languag', 26), ('compil optim', 19), ('code gener', 17), ('compil construct', 15), ('design implement', 14), ('optim compil', 14), ('sourc code', 14), ('gener code', 13), ('program transform', 12), ('copyright acm', 11)]\n",
      "Trigrams: [('higherord abstract syntax', 5), ('low power consumpt', 3), ('machin learn algorithm', 2), ('case studi demonstr', 2), ('experi show propos', 2), ('experiment show propos', 1), ('larg amount data', 1), ('secur multiparti comput', 1), ('support vector machin', 1), ('vector machin svm', 1)]\n",
      "\n",
      "The most frequent unigrams and bigrams for cluster 5.\n",
      "Unigrams: [('secur', 158), ('propos', 139), ('cryptographi', 108), ('encrypt', 101), ('comput', 91), ('key', 90), ('data', 83), ('perform', 82), ('scheme', 81), ('algorithm', 79)]\n",
      "Bigrams: [('internet thing', 27), ('ellipt curv', 24), ('encrypt scheme', 23), ('propos scheme', 22), ('thing iot', 21), ('curv cryptographi', 20), ('secret key', 18), ('data secur', 18), ('quantum comput', 17), ('iot devic', 16)]\n",
      "Trigrams: [('internet thing iot', 21), ('ellipt curv cryptographi', 20), ('curv cryptographi ecc', 10), ('random oracl model', 8), ('imag encrypt scheme', 7), ('gate array fpga', 7), ('institut standard technolog', 6), ('nation institut standard', 6), ('key agreement protocol', 6), ('secur multiparti comput', 5)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_labels = labels_list[i_best]\n",
    "features = tfidf_vectorizer.inverse_transform(tf_idf_vectors)\n",
    "clusters = {}\n",
    "\n",
    "for label, feature in zip(best_labels, features):\n",
    "    if label not in clusters:\n",
    "        clusters[label] = []\n",
    "    clusters[label].extend(feature)\n",
    "\n",
    "clusters = dict(sorted(clusters.items()))\n",
    "\n",
    "# Join similar clusters\n",
    "if not baseline:\n",
    "    thresh = 15\n",
    "    merger = set()\n",
    "    # Calculate percentage overlap of topical words between all clusters and add to merger if they are above thresh\n",
    "    for i in clusters.keys():\n",
    "        for j in clusters.keys():\n",
    "            if i != j:\n",
    "                set1 = set([w[0] for w in FreqDist([str(item) for item in clusters[i] if len(item.split()) == 1]).most_common(10)])\n",
    "                set1.update(set([w[0] for w in FreqDist([str(item) for item in clusters[i] if len(item.split()) == 2]).most_common(10)]))\n",
    "                set1.update(set([w[0] for w in FreqDist([str(item) for item in clusters[i] if len(item.split()) == 3]).most_common(10)]))\n",
    "                set2 = set([w[0] for w in FreqDist([str(item) for item in clusters[j] if len(item.split()) == 1]).most_common(10)])\n",
    "                set2.update(set([w[0] for w in FreqDist([str(item) for item in clusters[j] if len(item.split()) == 2]).most_common(10)]))\n",
    "                set2.update(set([w[0] for w in FreqDist([str(item) for item in clusters[j] if len(item.split()) == 3]).most_common(10)]))\n",
    "                intersection = set1.intersection(set2)\n",
    "                union = set1.union(set2)\n",
    "                if (len(intersection) / len(union)) * 100 >= thresh:\n",
    "                    merger.add(frozenset([i, j]))\n",
    "    # Go through clusters to be merged and merge them\n",
    "    for i in list(merger):\n",
    "        pair = list(i)\n",
    "        c1 = pair[0]\n",
    "        c2 = pair[1]\n",
    "        if (c1 in clusters) and (c2 in clusters):\n",
    "            if (c1 < c2):\n",
    "                clusters[c1].extend(clusters[c2])\n",
    "                clusters.pop(c2)\n",
    "    # Rename clusters to remove gaps from merged clusters\n",
    "    clust_name = 0\n",
    "    for clust in list(clusters.keys()):\n",
    "        clusters[clust_name] = clusters.pop(clust)\n",
    "        clust_name += 1\n",
    "\n",
    "\n",
    "for cluster in clusters:\n",
    "    unigrams = [str(item) for item in clusters[cluster] if len(item.split()) == 1]\n",
    "    bigrams = [str(item) for item in clusters[cluster] if len(item.split()) == 2]\n",
    "\n",
    "    unigram_freq = FreqDist(unigrams)\n",
    "    bigram_freq = FreqDist(bigrams)\n",
    "\n",
    "    print(f\"The most frequent unigrams and bigrams for cluster {cluster}.\")\n",
    "    print(f\"Unigrams: {unigram_freq.most_common(10)}\")\n",
    "    if not baseline:\n",
    "        trigrams = [str(item) for item in clusters[cluster] if len(item.split()) == 3]\n",
    "        trigram_freq = FreqDist(trigrams)\n",
    "        print(f\"Bigrams: {bigram_freq.most_common(10)}\")\n",
    "        print(f\"Trigrams: {trigram_freq.most_common(10)}\\n\")\n",
    "    else:\n",
    "        print(f\"Bigrams: {bigram_freq.most_common(10)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics for each cluster based on baseline results\n",
    "##### Cluster 0: Compiling, paper and computing (unclear)\n",
    "##### Cluster 1: Computer vision and neural networks\n",
    "##### Cluster 2: Security, cryptography, and encryption\n",
    "##### Cluster 3: Internet of Things (IoT) and data security\n",
    "##### Cluster 4: Programming languages (unclear)\n",
    "##### Cluster 5: Robotics (unclear)\n",
    "##### Cluster 6: Databases and information systems, specifcially relational databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics for each cluster based on improved results\n",
    "##### Cluster 0: Computer vision and neural networks\n",
    "##### Cluster 1: Databases and information systems, specifically relational databases\n",
    "##### Cluster 2: Quantum computing and cryptography\n",
    "##### Cluster 3: Robotics and machine learning\n",
    "##### Cluster 4: Programming languages and coding\n",
    "##### Cluster 5: Internet of things (IoT) and security/encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
