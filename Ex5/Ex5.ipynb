{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from string import punctuation\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\lmalv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lmalv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = 'data/scopusabstracts.csv'\n",
    "reader = open(data_path, 'r', encoding='utf-8')\n",
    "reader.readline() # skip header\n",
    "lines = reader.readlines()\n",
    "text = [i.split('#')[1] + ' ' + i.split('#')[2] for i in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "tokens_list = [word_tokenize(i) for i in text]\n",
    "\n",
    "# lowercasing\n",
    "lc_tokens_list = []    \n",
    "for i in tokens_list: \n",
    "    lc_tokens_list.append([token.lower() for token in i])\n",
    "\n",
    "# removing stopwords, punctuation, and numbers\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(punctuation)\n",
    "stop_words.add(\"...\")\n",
    "filtered_sentence = []    \n",
    "for i in lc_tokens_list: \n",
    "    filtered_sentence.append([token for token in i if token not in stop_words])\n",
    "filtered_sentence = [' '.join(i) for i in filtered_sentence]\n",
    "filtered_sentence = [re.sub(r'\\d+', '', sentence) for sentence in filtered_sentence]\n",
    "\n",
    "# stemming\n",
    "porter = PorterStemmer()\n",
    "stemmed_tokens_list = []\n",
    "for i in filtered_sentence:\n",
    "\tstemmed_tokens_list.append([porter.stem(j) for j in i.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ten rows of data after preprocessing:\n",
      "anomali detect wide area imageri geniş alan görüntülerind anomali tespiti studi detect anomali wide area imageri collect aircraft set anomali identifi anyth normal cours action purpos two differ data set use experi carri data set anomali detect convolut neural network model tri gener next imag use past imag design imag pre-process given model anomali detect perform compar estim imag true imag  \n",
      "person re-identif deep kronecker-product match group-shuffl random walk person re-identif re-id aim robustli measur visual affin person imag wide applic intellig surveil associ person imag across multipl camera gener treat imag retriev problem given probe person imag affin probe imag galleri imag pg affin use rank retriev galleri imag exist two main challeng effect solv problem person imag usual show signific variat differ person pose view angl spatial layout correspond person imag therefor vital inform tackl problem state-of-the-art method either ignor spatial variat util extra pose inform handl challeng exist person re-id method rank galleri imag consid pg affin ignor affin galleri imag gg affin affin could provid import clue accur galleri imag rank util post-process stage current method articl propos unifi end-to-end deep learn framework tackl two challeng handl viewpoint pose variat compar person imag propos novel kroneck product match oper match warp featur map differ person compar warp featur map result accur pg affin fulli util avail pg gg affin accur rank galleri person imag novel group-shuffl random walk oper propos kroneck product match group-shuffl random walk oper end-to-end trainabl shown improv learn visual featur integr deep learn framework propos approach outperform state-of-the-art method market- cuhk dukemtmc dataset demonstr effect gener abil propos approach code avail http //github.com/yantaoshen/kpm_rw_person_reid  \n",
      "crack detect imag masonri use cnn signific bodi research crack detect comput vision method concret asphalt less attent given masonri train convolut neural network cnn imag brick wall built laboratori environ test abil detect crack imag brick-and-mortar structur laboratori real-world imag taken internet also compar perform cnn varieti simpler classifi oper handcraft featur find cnn perform better domain adapt laboratori real-world imag simpl model howev also find perform significantli better perform revers domain adapt task simpl classifi train real-world imag test laboratori imag work demonstr abil detect crack imag masonri use varieti machin learn method provid guidanc improv reliabl model perform domain adapt crack detect masonri  \n",
      "toward energi effici code gener mobil phone use smartphon becom part everyday life last year devic help us mani area life sport job weather etc sometim also annoy batteri life time import find solut reduc energi consumpt smartphon one possibl method 'comput offload part process execut remot devic e.g cloud lot exampl alreadi shown comput offload reduc energi usag mobil devic howev amount energi save may differ decis make offload process control sever techniqu decis make theori base schedul theori paper go introduc new system call ecgm energi effici code gener mobil phone ecgm decid automat compil time task run smartphon task offload benefit system demonstr measur base energy-effici schedul techniqu  \n",
      "sub-polyhedr schedul use unit- two-variable-per-inequ polyhedra polyhedr compil success design implement complex loop nest optim parallel compil algorithm complex scalabl limit remain one import weak address use sub-polyhedr under-aproxim system constraint result affin schedul problem propos sub-polyhedr schedul techniqu use unit- two-variable-per-inequ u tvpi polyhedra techniqu reli simpl polynomi time algorithm under-approxim gener polyhedron u tvpi polyhedra modifi state-of-the-art pluto compil use schedul techniqu show major polybench . kernel under-approxim yield polyhedra non-empti solv under-approxim system lead asymptot gain complex show practic signific improv compar tradit lp solver also verifi code gener sub-polyhedr parallel prototyp match perform pluto-optim code under-approxim preserv feasibl copyright  \n",
      "extract multipl viewpoint model relat databas much time process mine project spent find understand data sourc extract event data need result fraction time spent actual appli techniqu discov control predict busi process moreov current process mine techniqu assum singl case notion howev real-lif process often differ case notion intertwin exampl event order handl process may refer custom order order line deliveri payment therefor propos use multipl viewpoint mvp model relat event object relat activ class requir event data much closer exist relat databas mvp model provid holist view process also allow extract classic event log use differ viewpoint way exist process mine techniqu use viewpoint without need new data extract transform provid toolchain allow discoveri mvp model annot perform frequenc inform relat databas moreov demonstr classic process mine techniqu appli select viewpoint  \n",
      "program result checker lexic analysi gnu c compil theori program result check establish well-suit method construct formal correct compil frontend never prove practic real-lif compil proof necessari establish result check method choic implement compil correctli show lexic analysi gnu c compil formal specifi check within theorem prover isabelle/hol util program check therebi demonstr formal specif verif techniqu abl handl real-lif compil  \n",
      "advanc visual object track algorithm base correl filter 基于相关滤波的视觉目标跟踪算法新进展 excel comprehens perform correl filter-bas track algorithm becom hotspot theoret research practic applic field visual object track despit mani studi still lack systemat analys exist correl filter-bas track algorithm level track framework therefor start basic framework object track algorithm characterist correl filter-bas track algorithm deepli analyz basic problem work stage present paper basi main technolog progress correl filter-bas track algorithm characterist correspond algorithm recent ten year summar typic correl filter-bas track algorithm evalu analyz final outstand issu urgent solv futur research direct correl filter-bas track algorithm discuss  \n",
      "studi variou databas model relat graph hybrid databas relat databas popular databas store variou type inform due ever-increas growth data becom hard maintain process databas graph model becom popular sinc store handl big data effici compar relat databas relat databas graph databas advantag disadvantag overcom limit combin make hybrid model paper discuss relat databas graph databas advantag applic also talk hybrid model  \n",
      "better report studi artifici intellig consort-ai beyond increas number studi artifici intellig ai publish dental oral scienc report also aspect studi suffer rang limit standard toward report like recent publish consolid standard report trial consort -ai extens help improv studi emerg field journal dental research jdr encourag author review reader adher standard notabl though wide rang aspect beyond report locat along variou step ai lifecycl consid conceiv conduct report evalu studi ai dentistri  \n",
      "\n",
      "The number of tokens after preprocessing is 10274.\n",
      "\n",
      "Most common words (total 10274):\n",
      "[('use', 1793), ('data', 1238), ('system', 1208), ('propos', 1082), ('model', 937), ('method', 880), ('comput', 868), ('robot', 806), ('imag', 792), ('perform', 774), ('base', 728), ('algorithm', 719), ('databas', 701), ('result', 684), ('secur', 665), ('paper', 635), ('approach', 621), ('compil', 602), ('applic', 593), ('design', 569), ('gener', 548), ('learn', 543), ('develop', 535), ('detect', 513), ('process', 512), ('.', 512), ('inform', 507), ('network', 505), ('present', 504), ('implement', 481), ('differ', 470), ('improv', 445), ('relat', 445), ('provid', 441), ('show', 437), ('optim', 437), ('techniqu', 430), ('time', 428), ('studi', 417), ('program', 417), ('evalu', 386), ('also', 385), ('effici', 380), ('work', 374), ('problem', 366), ('analysi', 365), ('object', 361), ('scheme', 358), ('research', 348), ('new', 348), ('featur', 347), ('control', 337), ('key', 336), ('structur', 333), ('compar', 332), ('requir', 331), ('vision', 327), ('two', 324), ('task', 320), ('framework', 316), ('encrypt', 313), ('effect', 303), ('howev', 303), ('technolog', 302), ('oper', 298), ('code', 298), ('one', 295), ('test', 294), ('languag', 293), ('set', 281), ('achiev', 280), ('quantum', 276), ('queri', 274), ('dataset', 272), ('environ', 270), ('train', 269), ('measur', 267), ('visual', 264), ('challeng', 261), ('complex', 259), ('exist', 258), ('accuraci', 255), ('function', 255), ('architectur', 252), ('cryptographi', 250), ('integr', 241), ('increas', 240), ('experi', 239), ('demonstr', 239), ('map', 237), ('multipl', 235), ('reduc', 235), ('construct', 235), (\"'s\", 233), ('deep', 232), ('number', 231), ('user', 231), ('estim', 226), ('transform', 226), ('attack', 223)]\n"
     ]
    }
   ],
   "source": [
    "# show processed data\n",
    "print('First ten rows of data after preprocessing:')\n",
    "for i in stemmed_tokens_list[:10]:\n",
    "\tfor j in i:\n",
    "\t\tprint(j,end=\" \")\n",
    "\tprint(\" \")\n",
    "\n",
    "# number of tokens\n",
    "uniques = np.unique([tok for doc in stemmed_tokens_list for tok in doc])\n",
    "print(\"\\nThe number of tokens after preprocessing is {}.\".format(len(uniques)))\n",
    "\n",
    "# check most frequent words\n",
    "listofall = [item for elem in stemmed_tokens_list for item in elem]\n",
    "freq = FreqDist(listofall)\n",
    "wnum = freq.B()\n",
    "print(\"\\nMost common words (total %d):\"%wnum)\n",
    "print(freq.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of samples is 1143 and the number of features is 1656.\n"
     ]
    }
   ],
   "source": [
    "cleaned_documents = [ ' '.join(i) for i in stemmed_tokens_list]\n",
    "tfidf_vectorizer = TfidfVectorizer(smooth_idf = False, ngram_range = (1, 2) , min_df = 0.01, max_df = 0.95)\n",
    "\n",
    "tfidf_vectorizer.fit(cleaned_documents)\n",
    "tf_idf_vectors = tfidf_vectorizer.transform(cleaned_documents)\n",
    "\n",
    "print(f\"\\nThe number of samples is {tf_idf_vectors.shape[0]} and the number of features is {tf_idf_vectors.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding best K for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Davies-Bouldin index is 5.2867 with K = 9.\n"
     ]
    }
   ],
   "source": [
    "k_values = range(3, 11)\n",
    "db_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters = k, random_state = 42)\n",
    "    labels = kmeans.fit_predict(tf_idf_vectors)\n",
    "\n",
    "    db_score = davies_bouldin_score(tf_idf_vectors.toarray(), labels)\n",
    "    db_scores.append(db_score)\n",
    "\n",
    "best_k = k_values[np.argmin(db_scores)]\n",
    "print(f\"The best Davies-Bouldin index is {min(db_scores):.4f} with K = {best_k}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
